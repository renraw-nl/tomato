"""Logger

Setup the logger based on the configuration data.
"""
import datetime
import logging
import logging.config
import sys
from pathlib import Path
from typing import Any, Final

import structlog
from pythonjsonlogger import jsonlogger

# _DT_FMT: Final[str] = "%Y-%m-%d %H:%M:%S.%fZ"
_DT_FMT: Final[str] = "%Y-%m-%d %H:%M:%S"
_LOG_LVL = "INFO"
_LOG_FMT: Final[
    str
] = "%(timestamp)s [%(levelname)s] %(name)s: %(message)s (%(pathname)s:%(lineno)d)"
_LOG_FILE: Final[Path] = Path.cwd() / datetime.datetime.today().strftime(
    f"{__package__} %Y-%m-%d.log"
)


class StuctlogJsonFormatter(jsonlogger.JsonFormatter):
    """JSON Formatter to have the output resember that of StructLog"""

    def add_fields(
        self,
        log_record: dict[str, Any],
        record: logging.LogRecord,
        message_dict: dict[str, Any],
    ) -> None:
        super(self.__class__, self).add_fields(log_record, record, message_dict)
        if not log_record.get("timestamp"):
            log_record["timestamp"] = datetime.datetime.utcfromtimestamp(
                record.created
            ).strftime(_DT_FMT)

        if log_record.get("level"):
            log_record["level"] = log_record["level"].upper()
        else:
            log_record["level"] = record.levelname


def init(
    data: dict, loglevel: str | int = "INFO", logfile_name: str | Path = None
) -> None:
    """Set up the logger using the passed `etc.data`."""

    # No need to run this twice, so save a few cycles.
    if structlog.is_configured():
        return

    if logfile_name and not isinstance(logfile_name, Path):
        logfile = Path(logfile_name).resolve()

    # get the default level or use the given one and convert it string to a level
    if not (loglevel := loglevel_from_str(loglevel)):
        loglevel = loglevel_from_str(data.get("logging.level", _LOG_LVL))

    # get the default or use the passed one
    if not logfile:
        logfile = data.get("logging.logfile", None)

    # Prepare the standard lib logger (logging.getLogger()) to show similar data as
    #  structlog. This is mainly of importance for logs generated by other
    #  packages/modules or before this method is called.
    # See: https://github.com/madzak/python-json-logger

    timestamper = structlog.processors.TimeStamper(fmt=_DT_FMT, utc=True)
    shared_processors = [
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.ExtraAdder(),
        timestamper,
    ]

    if sys.stderr.isatty():
        _init_cli_logger(loglevel, shared_processors, logfile)
    else:
        _init_container_logger(loglevel, shared_processors, logfile)


def _init_cli_logger(
    loglevel: str | int, shared_processors: dict[Any, Any], logfile: Path
) -> None:
    # formatter = jsonlogger.JsonFormatter(_LOG_FMT)
    # handler = logging.StreamHandler(sys.stdout)
    # handler.setFormatter(formatter)

    # root_logger = logging.getLogger()
    # root_logger.setLevel(loglevel)
    # root_logger.addHandler(handler)

    # structlog.configure(
    #     logger_factory=structlog.stdlib.LoggerFactory(),
    #     wrapper_class=structlog.stdlib.BoundLogger,
    #     cache_logger_on_first_use=True,
    #     context_class=dict,
    #     processors=shared_processors
    #     + [
    #         structlog.stdlib.PositionalArgumentsFormatter(),
    #         structlog.processors.StackInfoRenderer(),
    #         structlog.processors.format_exc_info,
    #         structlog.processors.UnicodeDecoder(),
    #         structlog.stdlib.render_to_log_kwargs,
    #         structlog.stdlib.ProcessorFormatter.wrap_for_formatter,
    #     ],
    # )
    logconfig: dict = {
        "version": 1,
        "disable_existing_loggers": False,
        "formatters": {
            "colored": {
                "()": structlog.stdlib.ProcessorFormatter,
                "processors": [
                    structlog.stdlib.ProcessorFormatter.remove_processors_meta,
                    structlog.dev.ConsoleRenderer(colors=True),
                ],
                "foreign_pre_chain": shared_processors,
            },
        },
        "handlers": {
            "default": {
                "level": loglevel,
                "class": "logging.StreamHandler",
                "formatter": "colored",
            },
        },
        "loggers": {
            "": {
                "handlers": [
                    "default",
                ],
                "level": loglevel,
                "propagate": True,
            },
        },
    }

    if logfile:
        logconfig["formatters"]["plain"] = {
            "()": structlog.stdlib.ProcessorFormatter,
            "processors": [
                structlog.stdlib.ProcessorFormatter.remove_processors_meta,
                structlog.dev.ConsoleRenderer(colors=False),
            ],
            "foreign_pre_chain": shared_processors,
        }
        logconfig["handlers"]["file"] = {
            "level": loglevel,
            "class": "logging.handlers.WatchedFileHandler",
            "filename": logfile,
            "formatter": "plain",
        }
        logconfig["loggers"][""]["handlers"].append("file")

    logging.config.dictConfig(logconfig)

    shared_processors = [
        structlog.stdlib.add_log_level,
    ] + shared_processors

    structlog.configure(
        processors=shared_processors
        + [
            structlog.stdlib.PositionalArgumentsFormatter(),
            structlog.processors.StackInfoRenderer(),
            structlog.processors.format_exc_info,
            structlog.stdlib.ProcessorFormatter.wrap_for_formatter,
        ],
        logger_factory=structlog.stdlib.LoggerFactory(),
        wrapper_class=structlog.stdlib.BoundLogger,
        cache_logger_on_first_use=True,
    )


def _init_container_logger(
    loglevel: str | int, shared_processors: dict, logfile: Path
) -> None:
    formatter = StuctlogJsonFormatter(_LOG_FMT)
    handler = logging.StreamHandler(sys.stdout)
    handler.setFormatter(formatter)

    root_logger = logging.getLogger()
    root_logger.setLevel(loglevel)
    root_logger.addHandler(handler)

    shared_processors = [
        structlog.stdlib.add_log_level,
    ] + shared_processors

    structlog.configure(
        logger_factory=structlog.stdlib.LoggerFactory(),
        # logger_factory=structlog.BytesLoggerFactory(),
        wrapper_class=structlog.make_filtering_bound_logger(loglevel),
        cache_logger_on_first_use=True,
        context_class=dict,
        processors=shared_processors
        + [
            structlog.contextvars.merge_contextvars,
            structlog.processors.format_exc_info,
            structlog.processors.UnicodeDecoder(),
            structlog.stdlib.render_to_log_kwargs,
            # structlog.processors.JSONRenderer(serializer=json.dumps),
        ],
    )


def loglevel_from_str(loglevel: str | int) -> int:
    return getattr(logging, loglevel.upper(), None)


def _extract_from_record(_, __, event_dict):
    """
    Extract thread and process names and add them to the event dict.
    """
    record = event_dict["_record"]
    event_dict["thread_name"] = record.threadName
    event_dict["process_name"] = record.processName

    return event_dict
